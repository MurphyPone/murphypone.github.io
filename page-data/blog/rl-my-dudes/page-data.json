{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/rl-my-dudes","webpackCompilationHash":"","result":{"data":{"markdownRemark":{"html":"<h2>The RL Dilemma</h2>\n<p>RL can be pretty cool sometimes. You don't have to know anything about physics but can program a robot to walk. You don't even have to know what the task you're trying to achieve is. You can just look at some numbers and find complex patterns in them that allow you to maximize some mystic external reward signal. It's all pretty magical at first glance, but if you really look at it, there are some glaring holes. Don't get me wrong. This is my future cause I suck at most other things so I need RL to keep getting better, but I can still rant about it every now and then.</p>\n<p>First things first, RL really can't do much. It can accomplish very cool things, but notice how those things tend to be very specific. Winning a board game has a clear objective. Teaching a robot to walk can be easily formulated to mean \"make this robot get as far away from its starting point as possible.\" It's easy to define what winning and losing looks like in these situations. But what about tasks that humans don't understand? Isn't tackling problems that humans aren't smart enough to comprehend the point of AI? As nice as that would be, we're a long way off from any reality that even comes close to containing sentient robots that can think for themselves.</p>\n<p>Us humans have been trying to tackle the credit assignment problem for some time, and our inability to do so is keeping RL from being the Les Mis style revolution against human intellectual dominance that I've been having recurring nightmares about. The credit assignment problem is stupidly simple to state: given a list of things you did in the past, what led to where you are now? This is basic cause and effect that humans are able to pick up relatively quickly. Touching red glowing things usually means we get burnt, jumping off something tall usually means you hurt your legs, and front flipping into a table of legos can cause irreparable damage unless you're a super humman. But as simple as this may seem for us, machines don't get it. We teach them using math, and math isn't always the best at defining relationships across time. It's not very often that a kindergarten student looks at a cause and effect worksheet and says \"ah, this must be the visual representation of a policy gradient\" because that would be a really fucking stupid thing for them to say. They don't know calculus, my dudes.</p>\n<p>But let's say we were able to somehow get machines to understand at a high level what cause and effect is and to accurately correlate their past experiences with their current situations. Even still, we wouldn't be able to take RL into the realm of problems in which the optimal outcome is unknown to humans. That's because RL needs a clearly defined objective. Even something as simple as \"Taking over the world = 1; Anything else = -1\" could theoretically train some massive policy network to take over the world if it had infinite time to train. What many human problems are, instead of being a binary \"this is good, this is bad\", are problems involving balance. If you have two different objectives but are unsure how to allot your resources to each, then you should be able to hand your problem off to a computer and have it figure it out. The balance of these different objectives all affect some unknown single objective that we want to maximize, but we don't really know what that is. We could in fact argue that humanity is one big inverse reinforcement learning cesspool trying to figure out what defines life's reward function.</p>\n<p>The problems with RL can be written out with mathematical expressions and complicated logic and symbols and all of that, but its biggest issue is simply that we don't even know what solving our problems looks like in the first place. Life's issues nowadays are so complex and intertwined that forming one overarching objective will probably always be impossible for humans. And if we don't even know how to maximize an unknown objective, then how can we teach machines to do that for us? Everything an RL agent learns is based on what we tell it. In some way, shape, or form, they are our students and we're trying to learn to communicate with them in the most efficient strain of math we can think of. But there's no way for us to communicate about problems that we can't even comprehend ourselves. So fuck this shit. Woop woop. We going out NFL style.</p>","frontmatter":{"date":"26 June, 2019","path":"/blog/rl-my-dudes","title":"Reinforcement Learning Kinda Sucks","author":"Brady H."},"fields":{"readingTime":{"text":"4 min read"}}}},"pageContext":{"isCreatedByStatefulCreatePages":false}}}